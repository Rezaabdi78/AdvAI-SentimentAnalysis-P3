{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "58d14e87",
      "metadata": {
        "id": "58d14e87"
      },
      "source": [
        "\n",
        "# Movie Review Sentiment Analysis\n",
        "This Jupyter Notebook details the implementation of a binary linear classifier for sentiment analysis of movie reviews.\n",
        "The classifier is developed using Python without external libraries like numpy or Scikit-learn.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "id": "dd665c4b",
      "metadata": {
        "id": "dd665c4b"
      },
      "outputs": [],
      "source": [
        "import requests\n",
        "\n",
        "def download_file(url, filename):\n",
        "    response = requests.get(url)\n",
        "    with open(filename, 'wb') as f:\n",
        "        f.write(response.content)\n",
        "\n",
        "util_url = 'https://raw.githubusercontent.com/Rezaabdi78/AdvAI-SentimentAnalysis-P3/main/util.py'\n",
        "train_data_url = 'https://raw.githubusercontent.com/Rezaabdi78/AdvAI-SentimentAnalysis-P3/main/polarity.train'\n",
        "validation_data_url = 'https://raw.githubusercontent.com/Rezaabdi78/AdvAI-SentimentAnalysis-P3/main/polarity.dev'\n",
        "grader_url = 'https://raw.githubusercontent.com/Rezaabdi78/AdvAI-SentimentAnalysis-P3/main/grader.py'\n",
        "graderUtil_url = 'https://raw.githubusercontent.com/Rezaabdi78/AdvAI-SentimentAnalysis-P3/main/graderUtil.py'\n",
        "submission_url = 'https://raw.githubusercontent.com/Rezaabdi78/AdvAI-SentimentAnalysis-P3/main/submission.py'\n",
        "\n",
        "download_file(util_url, 'util.py')\n",
        "download_file(train_data_url, 'polarity.train')\n",
        "download_file(validation_data_url, 'polarity.dev')\n",
        "download_file(grader_url, 'grader.py')\n",
        "download_file(graderUtil_url, 'graderUtil.py')\n",
        "download_file(submission_url, 'submission.py')\n",
        "\n",
        "\n",
        "import random\n",
        "from typing import Callable, Dict, List, Tuple, TypeVar\n",
        "\n",
        "from util import *\n",
        "\n",
        "FeatureVector = Dict[str, int]\n",
        "WeightVector = Dict[str, float]\n",
        "Example = Tuple[FeatureVector, int]"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "d79d81d7",
      "metadata": {
        "id": "d79d81d7"
      },
      "source": [
        "\n",
        "## Problem 3a: Feature Extraction\n",
        "**Objective:** Implement the `extractWordFeatures` function to convert a movie review (string) into a feature vector.\n",
        "Each word in the string is treated as a feature, and its frequency in the string is its corresponding value in the feature vector.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "id": "fed9473a",
      "metadata": {
        "id": "fed9473a"
      },
      "outputs": [],
      "source": [
        "\n",
        "def extractWordFeatures(x: str) -> dict:\n",
        "    words_list = x.split(' ')\n",
        "    occurance_counter = dict()\n",
        "    for word in words_list:\n",
        "        occurance_counter[word] = occurance_counter.get(word, 0) + 1\n",
        "    return occurance_counter\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "2f74e0c7",
      "metadata": {
        "id": "2f74e0c7"
      },
      "source": [
        "\n",
        "## Problem 3b: Stochastic Gradient Descent (SGD)\n",
        "**Objective:** Implement the `learnPredictor` function using stochastic gradient descent to minimize hinge loss for binary classification. This function iteratively updates a weight vector based on the training examples, aiming to reduce the classification error.\n",
        "\n",
        "**Methodology:**\n",
        "- SGD iteratively processes each training example and updates the model's weights.\n",
        "- Hinge loss is used as the loss function, which is suitable for binary classification tasks.\n",
        "- The function tracks training and validation errors to monitor the learning process.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "id": "619873c9",
      "metadata": {
        "id": "619873c9"
      },
      "outputs": [],
      "source": [
        "\n",
        "def learnPredictor(trainExamples, validationExamples, featureExtractor, numEpochs, eta):\n",
        "    weights = {}  # feature => weight\n",
        "\n",
        "    def predictor(x):\n",
        "        features = featureExtractor(x)\n",
        "        return 1 if dotProduct(features, weights) >= 0 else -1\n",
        "\n",
        "    for epoch in range(numEpochs):\n",
        "        for x, y in trainExamples:\n",
        "            extractedFeatures = featureExtractor(x)\n",
        "            if y * dotProduct(extractedFeatures, weights) < 1:\n",
        "                gradient = {feature: -y * value for feature, value in extractedFeatures.items()}\n",
        "                increment(weights, -eta, gradient)\n",
        "\n",
        "        trainError = evaluatePredictor(trainExamples, predictor)\n",
        "        validationError = evaluatePredictor(validationExamples, predictor)\n",
        "        print(f\"Epoch {epoch + 1}: Training error = {trainError}, Validation error = {validationError}\")\n",
        "\n",
        "    return weights\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "ccfb15e4",
      "metadata": {
        "id": "ccfb15e4"
      },
      "source": [
        "\n",
        "## Problem 3c: Generate Test Case\n",
        "**Objective:** Develope the `generateDataset` function to create synthetic examples that are correctly classified by a given weight vector. This function aids in verifying the effectiveness of the classifier.\n",
        "\n",
        "**Methodology:**\n",
        "- The function generates data points that align with the classifier's decision boundary.\n",
        "- The synthetic data should be representative of the types of examples the classifier will encounter.\n",
        "- This approach is useful for testing and debugging the classifier.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "id": "bfead33f",
      "metadata": {
        "id": "bfead33f"
      },
      "outputs": [],
      "source": [
        "\n",
        "def generateDataset(numExamples: int, weights: WeightVector) -> List[Example]:\n",
        "    random.seed(42)\n",
        "\n",
        "\n",
        "    def generateExample() -> Tuple[Dict[str, int], int]:\n",
        "        # BEGIN_YOUR_CODE (our solution is 3 lines of code, but don't worry if you deviate from this)\n",
        "        random_length = random.randint(0, len(weights))\n",
        "        phi = dict()\n",
        "        for i in range(random_length):\n",
        "            phi.update({key: value for key, value in random.choice([weights.items()])})\n",
        "        random_y = int(random.uniform(-2, 2))\n",
        "        y = random_y if random_y not in [0, -1] else 1\n",
        "        # END_YOUR_CODE\n",
        "        return phi, y\n",
        "\n",
        "    return [generateExample() for _ in range(numExamples)]\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "257bbf29",
      "metadata": {
        "id": "257bbf29"
      },
      "source": [
        "\n",
        "## Problem 3d: Character Features\n",
        "**Objective:** Implement the `extractCharacterFeatures` function to extract character n-gram features from a string. This approach considers sequences of characters, ignoring whitespace, to form features.\n",
        "\n",
        "**Methodology:**\n",
        "- Character n-grams provide a way to capture textual patterns at a finer granularity than whole words.\n",
        "- This method can be particularly effective for languages where word boundaries are less clear.\n",
        "- The function should count occurrences of each n-gram in the input string.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "id": "db152d36",
      "metadata": {
        "id": "db152d36"
      },
      "outputs": [],
      "source": [
        "\n",
        "def extractCharacterFeatures(n: int) -> Callable[[str], FeatureVector]:\n",
        "    def extract(x: str) -> Dict[str, int]:\n",
        "        # Example implementation (to be replaced with actual logic)\n",
        "        result = {}\n",
        "        x = x.replace(\" \", \"\")  # Remove whitespace\n",
        "        for i in range(len(x) - n + 1):\n",
        "            ngram = x[i:i+n]\n",
        "            result[ngram] = result.get(ngram, 0) + 1\n",
        "        return result\n",
        "    return extract\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "f1e9c4be",
      "metadata": {
        "id": "f1e9c4be"
      },
      "source": [
        "\n",
        "## Problem 3e: Testing Different Values of N\n",
        "**Objective:** Explore the impact of different values of 'n' in the `extractCharacterFeatures` function to determine the optimal length of n-grams that minimizes validation error.\n",
        "\n",
        "**Methodology:**\n",
        "- Testing different 'n' values helps in understanding how the granularity of n-grams affects model performance.\n",
        "- The goal is to find a balance where the n-grams are informative enough to aid classification but not so detailed as to overfit or become too sparse.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "id": "40fb3215",
      "metadata": {
        "id": "40fb3215"
      },
      "outputs": [],
      "source": [
        "\n",
        "def testValuesOfN():\n",
        "    # Example implementation (to be replaced with actual logic)\n",
        "    for n in range(1, 5):  # Example range of n\n",
        "        featureExtractor = extractCharacterFeatures(n)\n",
        "        # Code to train model and evaluate performance\n",
        "        print(f\"Testing with n = {n}, Performance: ...\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "id": "dbba7bb9",
      "metadata": {
        "id": "dbba7bb9"
      },
      "outputs": [],
      "source": [
        "# Run grader.py script\n",
        "!python grader.py\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Problem 3e: Experimenting with Different Values of N\n",
        "\n",
        "### Experimental Setup\n",
        "Run the linear predictor with the `extractCharacterFeatures` feature extractor for different values of `n`. We aim to find the value of `n` that produces the smallest validation error.\n"
      ],
      "metadata": {
        "id": "J3uyKdvdV84O"
      },
      "id": "J3uyKdvdV84O"
    },
    {
      "cell_type": "code",
      "source": [
        "trainExamples = readExamples('polarity.train')\n",
        "validationExamples = readExamples('polarity.dev')\n",
        "numEpochs = 20\n",
        "eta = 0.01\n",
        "\n",
        "\n",
        "def run_character_feature_experiment(trainExamples, validationExamples, n_values):\n",
        "    for n in n_values:\n",
        "        featureExtractor = extractCharacterFeatures(n)\n",
        "        weights = learnPredictor(trainExamples, validationExamples, featureExtractor, numEpochs, eta)\n",
        "        validationError = evaluatePredictor(validationExamples, lambda x: (1 if dotProduct(featureExtractor(x), weights) >= 0 else -1))\n",
        "        print(f'n = {n}: Validation Error = {validationError}')\n",
        "\n",
        "n_values = [1, 2, 3, 4, 5]  # Example range of n\n",
        "run_character_feature_experiment(trainExamples, validationExamples, n_values)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7iPVlBflV_lb",
        "outputId": "afe2a67b-2e72-415f-fad8-8e207ac45b02"
      },
      "id": "7iPVlBflV_lb",
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Read 3554 examples from polarity.train\n",
            "Read 3554 examples from polarity.dev\n",
            "Epoch 1: Training error = 0.47326955543050087, Validation error = 0.4935284186831739\n",
            "Epoch 2: Training error = 0.449634214969049, Validation error = 0.4766460326392797\n",
            "Epoch 3: Training error = 0.44034890264490717, Validation error = 0.467923466516601\n",
            "Epoch 4: Training error = 0.4296567248171075, Validation error = 0.4631401238041643\n",
            "Epoch 5: Training error = 0.4569499155880698, Validation error = 0.48086662915025324\n",
            "Epoch 6: Training error = 0.4662352279122116, Validation error = 0.4879009566685425\n",
            "Epoch 7: Training error = 0.44991558806978055, Validation error = 0.475801913337085\n",
            "Epoch 8: Training error = 0.4752391671356218, Validation error = 0.4926842993809792\n",
            "Epoch 9: Training error = 0.4651097355092853, Validation error = 0.48480585256049524\n",
            "Epoch 10: Training error = 0.4448508722566123, Validation error = 0.4583567810917276\n",
            "Epoch 11: Training error = 0.4617332583005065, Validation error = 0.4887450759707372\n",
            "Epoch 12: Training error = 0.4606077658975802, Validation error = 0.47073719752391674\n",
            "Epoch 13: Training error = 0.4597636465953855, Validation error = 0.4819921215531795\n",
            "Epoch 14: Training error = 0.44879009566685424, Validation error = 0.48142937535171637\n",
            "Epoch 15: Training error = 0.47017445132245356, Validation error = 0.49380979178390544\n",
            "Epoch 16: Training error = 0.4428812605514913, Validation error = 0.47017445132245356\n",
            "Epoch 17: Training error = 0.4501969611705121, Validation error = 0.4766460326392797\n",
            "Epoch 18: Training error = 0.45807540799099605, Validation error = 0.4803038829487901\n",
            "Epoch 19: Training error = 0.4434440067529544, Validation error = 0.46933033202025887\n",
            "Epoch 20: Training error = 0.4586381541924592, Validation error = 0.4845244794597636\n",
            "n = 1: Validation Error = 0.4845244794597636\n",
            "Epoch 1: Training error = 0.3441193021947102, Validation error = 0.41305571187394485\n",
            "Epoch 2: Training error = 0.3249859313449634, Validation error = 0.40433314575126617\n",
            "Epoch 3: Training error = 0.2965672481710748, Validation error = 0.3925154755205402\n",
            "Epoch 4: Training error = 0.3128868880135059, Validation error = 0.40630275745638716\n",
            "Epoch 5: Training error = 0.317388857625211, Validation error = 0.4020821609454136\n",
            "Epoch 6: Training error = 0.3092290377039955, Validation error = 0.4060213843556556\n",
            "Epoch 7: Training error = 0.34271243669105234, Validation error = 0.42065278559369723\n",
            "Epoch 8: Training error = 0.3148564997186269, Validation error = 0.4113674732695554\n",
            "Epoch 9: Training error = 0.343556555993247, Validation error = 0.4209341586944288\n",
            "Epoch 10: Training error = 0.3497467642093416, Validation error = 0.42121553179516036\n",
            "Epoch 11: Training error = 0.351435002813731, Validation error = 0.41840180078784467\n",
            "Epoch 12: Training error = 0.3429938097917839, Validation error = 0.4234665166010129\n",
            "Epoch 13: Training error = 0.3044456949915588, Validation error = 0.4006752954417558\n",
            "Epoch 14: Training error = 0.32583005064715814, Validation error = 0.40911648846370285\n",
            "Epoch 15: Training error = 0.3390545863815419, Validation error = 0.4172763083849184\n",
            "Epoch 16: Training error = 0.30697805289814295, Validation error = 0.40574001125492404\n",
            "Epoch 17: Training error = 0.3759144625773776, Validation error = 0.4361283061339336\n",
            "Epoch 18: Training error = 0.3297692740574001, Validation error = 0.41755768148565\n",
            "Epoch 19: Training error = 0.3441193021947102, Validation error = 0.42234102419808667\n",
            "Epoch 20: Training error = 0.3072594259988745, Validation error = 0.40433314575126617\n",
            "n = 2: Validation Error = 0.40433314575126617\n",
            "Epoch 1: Training error = 0.21834552616769837, Validation error = 0.3562183455261677\n",
            "Epoch 2: Training error = 0.15081598199212154, Validation error = 0.3337084974676421\n",
            "Epoch 3: Training error = 0.12099043331457512, Validation error = 0.3337084974676421\n",
            "Epoch 4: Training error = 0.07343837929093978, Validation error = 0.31541924592009\n",
            "Epoch 5: Training error = 0.06893640967923466, Validation error = 0.3221722003376477\n",
            "Epoch 6: Training error = 0.04642656162070906, Validation error = 0.3176702307259426\n",
            "Epoch 7: Training error = 0.032357906584130555, Validation error = 0.3134496342149691\n",
            "Epoch 8: Training error = 0.030106921778277996, Validation error = 0.31513787281935846\n",
            "Epoch 9: Training error = 0.023916713562183455, Validation error = 0.317388857625211\n",
            "Epoch 10: Training error = 0.017445132245357344, Validation error = 0.3120427687113112\n",
            "Epoch 11: Training error = 0.013787281935846933, Validation error = 0.3134496342149691\n",
            "Epoch 12: Training error = 0.011817670230725942, Validation error = 0.3185143500281373\n",
            "Epoch 13: Training error = 0.0075970737197523916, Validation error = 0.31823297692740576\n",
            "Epoch 14: Training error = 0.0075970737197523916, Validation error = 0.31907709622960045\n",
            "Epoch 15: Training error = 0.007034327518289252, Validation error = 0.31541924592009\n",
            "Epoch 16: Training error = 0.005627462014631401, Validation error = 0.319358469330332\n",
            "Epoch 17: Training error = 0.004783342712436691, Validation error = 0.3176702307259426\n",
            "Epoch 18: Training error = 0.004220596510973551, Validation error = 0.3207653348339899\n",
            "Epoch 19: Training error = 0.004220596510973551, Validation error = 0.31654473832301633\n",
            "Epoch 20: Training error = 0.0028137310073157004, Validation error = 0.3159819921215532\n",
            "n = 3: Validation Error = 0.3159819921215532\n",
            "Epoch 1: Training error = 0.09088351153629713, Validation error = 0.27940348902644907\n",
            "Epoch 2: Training error = 0.027855936972425437, Validation error = 0.28137310073157007\n",
            "Epoch 3: Training error = 0.009003939223410242, Validation error = 0.27940348902644907\n",
            "Epoch 4: Training error = 0.005627462014631401, Validation error = 0.27771525042205963\n",
            "Epoch 5: Training error = 0.0028137310073157004, Validation error = 0.27884074282498594\n",
            "Epoch 6: Training error = 0.0019696117051209903, Validation error = 0.27884074282498594\n",
            "Epoch 7: Training error = 0.0008441193021947102, Validation error = 0.28165447383230163\n",
            "Epoch 8: Training error = 0.0005627462014631402, Validation error = 0.27884074282498594\n",
            "Epoch 9: Training error = 0.0002813731007315701, Validation error = 0.2810917276308385\n",
            "Epoch 10: Training error = 0.0, Validation error = 0.28165447383230163\n",
            "Epoch 11: Training error = 0.0, Validation error = 0.28081035453010694\n",
            "Epoch 12: Training error = 0.0, Validation error = 0.2799662352279122\n",
            "Epoch 13: Training error = 0.0, Validation error = 0.27884074282498594\n",
            "Epoch 14: Training error = 0.0, Validation error = 0.28165447383230163\n",
            "Epoch 15: Training error = 0.0, Validation error = 0.28165447383230163\n",
            "Epoch 16: Training error = 0.0, Validation error = 0.27968486212718063\n",
            "Epoch 17: Training error = 0.0, Validation error = 0.2824985931344963\n",
            "Epoch 18: Training error = 0.0, Validation error = 0.2830613393359595\n",
            "Epoch 19: Training error = 0.0, Validation error = 0.28165447383230163\n",
            "Epoch 20: Training error = 0.0, Validation error = 0.28418683173888576\n",
            "n = 4: Validation Error = 0.28418683173888576\n",
            "Epoch 1: Training error = 0.0498030388294879, Validation error = 0.27884074282498594\n",
            "Epoch 2: Training error = 0.010973550928531233, Validation error = 0.27968486212718063\n",
            "Epoch 3: Training error = 0.0022509848058525606, Validation error = 0.2734946539110861\n",
            "Epoch 4: Training error = 0.0022509848058525606, Validation error = 0.27124366910523356\n",
            "Epoch 5: Training error = 0.0008441193021947102, Validation error = 0.26927405740011257\n",
            "Epoch 6: Training error = 0.0008441193021947102, Validation error = 0.2723691615081598\n",
            "Epoch 7: Training error = 0.0002813731007315701, Validation error = 0.27743387732132807\n",
            "Epoch 8: Training error = 0.0, Validation error = 0.2743387732132808\n",
            "Epoch 9: Training error = 0.0, Validation error = 0.27687113111986494\n",
            "Epoch 10: Training error = 0.0, Validation error = 0.2751828925154755\n",
            "Epoch 11: Training error = 0.0, Validation error = 0.2743387732132808\n",
            "Epoch 12: Training error = 0.0, Validation error = 0.27546426561620707\n",
            "Epoch 13: Training error = 0.0, Validation error = 0.2732132808103545\n",
            "Epoch 14: Training error = 0.0, Validation error = 0.2726505346088914\n",
            "Epoch 15: Training error = 0.0, Validation error = 0.2732132808103545\n",
            "Epoch 16: Training error = 0.0, Validation error = 0.2734946539110861\n",
            "Epoch 17: Training error = 0.0, Validation error = 0.2737760270118177\n",
            "Epoch 18: Training error = 0.0, Validation error = 0.2734946539110861\n",
            "Epoch 19: Training error = 0.0, Validation error = 0.2737760270118177\n",
            "Epoch 20: Training error = 0.0, Validation error = 0.2737760270118177\n",
            "n = 5: Validation Error = 0.2737760270118177\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Observations and Analysis\n",
        "Through the experiment, we observed that the smallest validation error was achieved with n=5. This suggests that character n-grams of this length capture essential features of the text effectively, balancing specificity and generalization. Such n-grams can encapsulate partial words or word segments, which might be more robust to variations in word forms than whole words.\n",
        "\n",
        "### Practical Example\n",
        "Consider the review: \"Unbelievably thrilling!\" In this case, character n-grams might outperform word features as they can capture the essence of compounded words like 'unbelievably'. These n-grams can detect patterns in character sequences that transcend traditional word boundaries, making them more effective for such text."
      ],
      "metadata": {
        "id": "9o_MEEOgXjkA"
      },
      "id": "9o_MEEOgXjkA"
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Conclusion\n",
        "\n",
        "This notebook presented a detailed exploration of building a binary linear classifier for sentiment analysis of movie reviews. Key highlights and learnings are:\n",
        "\n",
        "- **Feature Extraction (Problem 3a):** We implemented a method to extract word features from text, demonstrating the importance of feature representation in machine learning models.\n",
        "- **Stochastic Gradient Descent (Problem 3b):** The application of SGD for hinge loss minimization highlighted the iterative nature of optimizing classification models.\n",
        "- **Synthetic Test Cases (Problem 3c):** Generating synthetic data provided insights into model behavior and helped in model validation.\n",
        "- **Character Features (Problem 3d and 3e):** Experimentation with character n-grams revealed their effectiveness in capturing linguistic patterns, especially in scenarios where traditional word boundaries are less distinct.\n",
        "\n",
        "Overall, this exercise emphasized the significance of feature selection, the impact of algorithmic choices in model performance, and the value of rigorous testing. These components are crucial in developing robust machine learning models for natural language processing tasks.\n"
      ],
      "metadata": {
        "id": "2h0I0dJfYmCs"
      },
      "id": "2h0I0dJfYmCs"
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.12.0"
    },
    "colab": {
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}